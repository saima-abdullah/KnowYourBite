
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline  # Use imblearn's pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,ConfusionMatrixDisplay,RocCurveDisplay,classification_report, f1_score
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
import pickle

#  oversampling 
from imblearn.over_sampling import SMOTE





df = pd.read_csv('../data/final_data.csv')
df.head()





df['health_label'] = df['health_label'].map({'healthy':0,'unhealthy':1})


X = df['processed_ingredients']
y= df['health_label']






y.value_counts(normalize = True)# unbalance class so we need to stratify





X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,stratify=y)





logr_pipeline = Pipeline(
    [
        ('cvec',CountVectorizer()),

        ('smote', SMOTE(random_state=42)),     # Oversampling step
        ('logr',LogisticRegression(random_state=42,max_iter=500))
]
)
grid_params = {
    'cvec__ngram_range': [(1, 1), (1, 2)],  # Unigrams and bigrams
    'cvec__max_df': [0.75, 0.85, 1.0],      # Ignore very frequent words
    'cvec__min_df': [1, 2, 5],              # Ignore very infrequent words
    'cvec__max_features': [None, 1000,5000, 10000],  # Limit on the number of features

    'logr__C': [0.01, 0.1, 1.0, 10],        # Regularization strength
    'logr__penalty': ['l2'],                # L2 regularization (Ridge)
    'logr__solver': ['lbfgs', 'liblinear']  # Solvers suitable for small datasets
}

gs_logr = GridSearchCV(logr_pipeline,param_grid=grid_params,n_jobs=-1)
gs_logr.fit(X_train,y_train)
print(f' Best parameters:{gs_logr.best_params_}')
print(f' Best score:{gs_logr.best_score_}')


gs_logr.best_estimator_.named_steps['logr'].intercept_
gs_logr.best_estimator_.named_steps['logr'].coef_



#Logistic Regression Coefficients 
print(f"Logistic Regression Intercept: {gs_logr.best_estimator_.named_steps['logr'].intercept_}")
print(f"Logistic Regression Coefficients: {gs_logr.best_estimator_.named_steps['logr'].coef_}")
coefficients=gs_logr.best_estimator_.named_steps['logr'].coef_.ravel()
vectorizer = gs_logr.best_estimator_.named_steps['cvec']
feature_names = vectorizer.get_feature_names_out()
coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
coefficients_df = coefficients_df.sort_values(by='Coefficient', ascending=False)
coefficients_df.head(),coefficients_df.tail(),
top_features = coefficients_df.head()
bottom_features = coefficients_df.tail()


print(f"Top 5 positive coefficients:\n{top_features}")
print(f"Top 5 negative coefficients:\n{bottom_features}")


preds = gs_logr.predict(X_test)

logr_train_accuracy = round(gs_logr.score(X_train, y_train), 2)
logr_test_accuracy = round(gs_logr.score(X_test, y_test), 2)

logr_train_misclassification_rate = round(1 - logr_train_accuracy, 2)
logr_test_misclassification_rate = round(1 - logr_test_accuracy, 2)

print(f'Training Accuracy of Logistic Regression with Count Vectorizer: {logr_train_accuracy}')
print(f'Testing Accuracy of Logistic Regression with Count Vectorizer: {logr_test_accuracy}')

print(f'Training Misclassification Rate: {logr_train_misclassification_rate}')
print(f'Testing Misclassification Rate: {logr_test_misclassification_rate}')
#specificity

tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()

logr_spec = tn / (tn + fp)
print(f'Logistic Regression specificity: {round(logr_spec,2)}')
# recall

logr_recall = tp / (tp + fn)

print(f'Logistic Regression recall: {round(logr_recall,2)}')
logr_f1_not_healthy = f1_score(y_test, preds, pos_label=1)
logr_f1_healthy = f1_score(y_test, preds, pos_label=0)

print(f'Logistic Regression F1_score unhealthy: {round(logr_f1_healthy,2)}')
print(f'Logistic Regression F1_score healthy : {round(logr_f1_not_healthy,2)}')


#classification report
print(classification_report(y_test, preds))


ConfusionMatrixDisplay.from_estimator(gs_logr,X_test,y_test);
RocCurveDisplay.from_estimator(gs_logr,X_test,y_test);








pipe_nb =Pipeline(
    [
        ('tf-idf',TfidfVectorizer()),
        ('smote', SMOTE(random_state=42)), #for class imbalance
        ('nb',MultinomialNB())
    ]
)


grid_params = {
    'tf-idf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'tf-idf__stop_words': [None, 'english'],
    'tf-idf__max_features': range(2000, 4001, 1000),
    'tf-idf__min_df': [2, 4],
    'tf-idf__max_df': np.linspace(0.9, 0.99, 5),
    'nb__alpha': [0.1, 0.5, 1.0]            # Smoothing parameter for Naive Bayes
   
}

gs_nb = GridSearchCV(pipe_nb, grid_params, n_jobs=-1, cv=5)
gs_nb.fit(X_train, y_train)

print(f' Best parameters:{gs_nb.best_params_}')
print(f' Best score:{gs_nb.best_score_}')


pred_nb = gs_nb.predict(X_test)

nb_train_accuracy = round(gs_nb.score(X_train, y_train), 2)
nb_test_accuracy = round(gs_nb.score(X_test, y_test), 2)

nb_train_misclassification_rate = round(1 - nb_train_accuracy, 2)
nb_test_misclassification_rate = round(1 - nb_test_accuracy, 2)

print(f'Training Accuracy of Naive bayes  with TF-IDF Vectorizer: {nb_train_accuracy}')
print(f'Testing Accuracy of Naive bayes  with TF-IDF Vectorizer: {nb_test_accuracy}')

print(f'Training Misclassification Rate: {nb_train_misclassification_rate}')
print(f'Testing Misclassification Rate: {nb_test_misclassification_rate}')
#  cross-validation 
cross_val_scores = cross_val_score(gs_nb.best_estimator_, X_train, y_train, cv=5, n_jobs=-1)

#  mean cross-validation score
mean_cross_val_score = cross_val_scores.mean()
print(f'cross val score of Naive Bayes :{mean_cross_val_score}')

#specificity

tn, fp, fn, tp = confusion_matrix(y_test, pred_nb).ravel()

nb_spec = tn / (tn + fp)
print(f'Naive Bayes specificity: {round(nb_spec,2)}')
# recall

nb_recall = tp / (tp + fn)

print(f'Naive Bayes recall: {round(nb_recall,2)}')
nb_f1_not_healthy = f1_score(y_test, pred_nb, pos_label=1)
nb_f1_healthy = f1_score(y_test, pred_nb, pos_label=0)


print(f'Naive bayes F1_score not healthy: {round(nb_f1_not_healthy,2)}')
print(f'Naive bayes  F1_score healthy : {round(nb_f1_healthy,2)}')


#classification report
print(classification_report(y_test, pred_nb))


ConfusionMatrixDisplay.from_estimator(gs_nb,X_test,y_test);
RocCurveDisplay.from_estimator(gs_nb,X_test,y_test);








## Gradient Boosting to takle class imbalance

pipe_gb = Pipeline(
    [
        ('tfidf', TfidfVectorizer()),  
        ('gb', GradientBoostingClassifier(random_state=42))  
    ]
)

#  hyperparameter grid
gb_best_params = {
    'gb__n_estimators': [10, 100],       # Number of trees
    'gb__max_depth': [None, 1, 2, 3],   # Depth of trees
    'gb__learning_rate': [0.1, 1, 10],  # Learning rate
}


gs = GridSearchCV(pipe_gb, param_grid=gb_best_params, cv=5, n_jobs=-1, scoring='accuracy')

# 
gs.fit(X_train, y_train)

# best parameters and score
print("Best Parameters:", gs.best_params_)
print("Best Score:", gs.best_score_)


pred_gb = gs.predict(X_test)

gb_train_accuracy = round(gs.score(X_train, y_train), 2)
gb_test_accuracy = round(gs.score(X_test, y_test), 2)

gb_train_misclassification_rate = round(1 - gb_train_accuracy, 2)
gb_test_misclassification_rate = round(1 - gb_test_accuracy, 2)

print(f'Training Accuracy of Graidient Boosting  with TF-IDF Vectorizer: {gb_train_accuracy}')
print(f'Testing Accuracy of Graidient Boosting  with TF-IDF Vectorizer: {gb_test_accuracy}')

print(f'Training Misclassification Rate: {gb_train_misclassification_rate}')
print(f'Testing Misclassification Rate: {gb_test_misclassification_rate}')
# cross-validation
cross_val_scores = cross_val_score(gs.best_estimator_, X_train, y_train, cv=5, n_jobs=-1)

#  mean cross-validation score
mean_cross_val_score = cross_val_scores.mean()
print(f'cross val score of Graidient Boosting :{mean_cross_val_score}')

#specificity

tn, fp, fn, tp = confusion_matrix(y_test, pred_gb).ravel()

gb_spec = tn / (tn + fp)
print(f'Naive Bayes specificity: {round(gb_spec,2)}')
# recall

gb_recall = tp / (tp + fn)

print(f'Naive Bayes recall: {round(gb_recall,2)}')
gb_f1_not_healthy= f1_score(y_test, pred_gb, pos_label=1)
gb_f1_healthy = f1_score(y_test, pred_gb, pos_label=0)


print(f'Graidient Boosting F1_score not healthy: {round(gb_f1_not_healthy,2)}')
print(f'Graidient Boosting  F1_score healthy : {round(gb_f1_healthy,2)}')


#classification report
print(classification_report(y_test, pred_gb))


ConfusionMatrixDisplay.from_estimator(gs,X_test,y_test);
RocCurveDisplay.from_estimator(gs,X_test,y_test);





#save the best model which is gradient boosting
with open('../models/foodclassifier.pkl', 'wb') as pickle_out:
    pickle.dump(gs.best_estimator_, pickle_out)







