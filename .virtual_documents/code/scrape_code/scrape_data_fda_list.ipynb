# Base URL of the page
base_url = "https://www.hfpappexternal.fda.gov/scripts/fdcc/index.cfm?set=FoodSubstances&sort=Sortterm_ID&order=ASC&startrow=1&type=basic&search=DELISTED"

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd 

response = requests.get(base_url)
if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # table
    table = soup.find('table', {'id': 'summaryTable'})
    
    #  table headers
    headers = [header.text.strip() for header in table.find('thead').find_all('th')]
    print(headers)
    
    #  table rows
    rows = []
    for row in table.find('tbody').find_all('tr'):
        cells = row.find_all('td')
        row_data = [cell.text.strip() for cell in cells]
        rows.append(row_data)
        print(rows)

   
    
   
    df = pd.DataFrame(rows, columns=headers)
    
    # saving data 
    output_file = "../data/FDA_Delisted_Additives.csv"
    df.to_csv(output_file, index=False)
    
    result_message = f"Data scraped successfully! The table has been saved to {output_file}."
else:
    result_message = f"Failed to fetch the page. Status code: {response.status_code}"

result_message
 



